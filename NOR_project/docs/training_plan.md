# Training Pipeline Outline

## Objectives
- Implement a PyTorch 3D CNN that consumes 5-frame clips (channels-first, shape `(C, T, H, W)` with `C=5` after auxiliary channels).
- Train for 15 epochs using the balanced sequence metadata generated earlier, leveraging Apple Silicon MPS when available.
- Preserve original 480×480 resolution while applying consistent per-clip augmentations.

## Data Flow
1. Load `balanced_sequences.csv` (generated by `create_balanced_sequences.py`).
2. For each row, parse `frame_paths` (pipe-separated absolute paths) and `frame_numbers`.
3. Retrieve per-frame interaction labels from `B004_per_frame_with_crops.csv` to build a binary prior mask for auxiliary channel 2.
4. Build tensors:
   - RGB stack → `(3, T, H, W)`
   - Temporal position encoding → `(1, T, H, W)`
   - Interaction prior mask → `(1, T, H, W)`
   - Concatenate to `(5, T, H, W)`.
5. Apply deterministic clip-level augmentations (flip, colour jitter, affine) when training.
6. Optional temporal dropout randomly replaces one neighbour frame with the centre frame.

## Model Skeleton
- **Stem**: `Conv3d(5 → 32, kernel=(3,7,7), stride=(1,2,2))`, BN, GELU, `MaxPool3d((1,2,2))`.
- **Residual stages**: three bottleneck-style blocks with output widths 64, 128, 256; spatial strides (1,2,2) for stages 2 and 3.
- **Temporal attention**: global average pool to `(B, C, T)`, apply attention weights via linear layer with softmax; enforce centre-frame minimum weight via hinge penalty.
- **Head**: concatenate attention context and centre feature → MLP (256 → 64 → 1) with GELU + dropout.

## Training Loop
- Loss: binary focal loss (`γ=1.5`, `α=0.5`) + `λ=0.1` centre-attention hinge penalty.
- Optimiser: AdamW (`lr=2e-4`, weight decay `1e-4`), cosine schedule with warmup 5 epochs.
- Splitting strategy: sort by `centre_frame`, use 80%/10%/10% contiguous split to reduce leakage.
- Metrics per epoch: loss, accuracy, precision, recall, F1, AUC if available (fallback to PR metrics using torch for now).
- Checkpoint best validation balanced accuracy to `models/interaction_cnn.pt`.
- Support `--max-steps` and `--dry-run` so local smoke tests avoid long runs.

## Device Handling
- Prefer MPS (`torch.backends.mps.is_available()`).
- Fallback sequence: CUDA → CPU. Print chosen device at runtime.

## Outputs
- `outputs/metrics.json` accumulating epoch metrics.
- `outputs/checkpoints/best.pt` and final `last.pt`.
- TensorBoard optional (TODO future work).
